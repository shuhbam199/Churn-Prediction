# -*- coding: utf-8 -*-
"""Ola_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KniEhjh8Rveg5Rst5jX7k9TlQKsZwRFn

<a href="https://colab.research.google.com/github/shuhbam199/FebGithub/blob/main/Ola_churn.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""



"""Problem Statement

Recruiting and retaining drivers is seen by industry watchers as a tough battle for Ola. Churn among drivers is high and it’s very easy for drivers to stop working for the service on the fly or jump to Uber depending on the rates.

As the companies get bigger, the high churn could become a bigger problem. To find new drivers, Ola is casting a wide net, including people who don’t have cars for jobs. But this acquisition is really costly. Losing drivers frequently impacts the morale of the organization and acquiring new drivers is more expensive than retaining existing ones.

Predict whether a driver will be leaving the company or not based on their attributes like

Demographics (city, age, gender etc.)
Tenure information (joining date, Last Date)
Historical data regarding the performance of the driver (Quarterly rating, Monthly business acquired, grade, Income)
"""



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score
import shap
import xgboost as xgb

df = pd.read_csv('ola.csv')

df.info()

"""null in lastworking date why ?

##EDA
"""

df.isna().sum()/len(df)*100

"""last working date indicates churn"""

df.groupby(['Driver_ID'])['Grade'].nunique().nlargest(5)

"""change in grade over a period of time"""

df[df['Driver_ID'] == 26]

from sklearn.impute import KNNImputer

df.select_dtypes(np.number)

df.select_dtypes(include = ['object']).columns.values

df.select_dtypes(include=['object'])

df['City_encoded'] = df.groupby('City')['Grade'].transform('mean')
#encoding

df['City_encoded'] = np.round(df['City_encoded'],3)

df['Dateofjoining'] = pd.to_datetime(df['Dateofjoining'])
df['LastWorkingDate'] = pd.to_datetime(df['LastWorkingDate'], errors='coerce')
df['MMM-YY'] = pd.to_datetime(df['MMM-YY'])
# converting to date type

df.groupby(['Driver_ID'])['Joining Designation'].nunique().nlargest(5)
#checking if joining designation is constant

"""##Filling null with KNN"""

from sklearn.impute import KNNImputer

KNN = KNNImputer(n_neighbors= 5, metric = 'nan_euclidean', weights = 'distance')

df2 = df.copy()

df2.drop('City', axis=1, inplace = True)

df2.drop('Unnamed: 0', axis = 1, inplace=True)

df3 = df2.select_dtypes(np.number)

df3_imputed =  pd.DataFrame(KNN.fit_transform(df3), columns = df3.columns)

df4 = df2.select_dtypes(exclude = [np.number])

df_new = pd.concat([df4, df3_imputed], axis =1)

df_new['Churn'] = df_new['LastWorkingDate'].notna().map({True:1, False:0})
##churn = 1, else 0

"""## Feature Engineering"""

df_new['Last_grade'] = df_new.groupby('Driver_ID')['Grade'].transform('last')

df_new['first_grade'] = df_new.groupby('Driver_ID')['Grade'].transform('first')

df_new['Last_des'] = df_new.groupby('Driver_ID')['Joining Designation'].transform('last')
df_new['first_des'] = df_new.groupby('Driver_ID')['Joining Designation'].transform('first')

df_new[df_new['Last_des'] != df_new['first_des']]

"""means designation is same"""

df_new['Last_rat'] = df_new.groupby('Driver_ID')['Quarterly Rating'].transform('last')
df_new['first_rat'] = df_new.groupby('Driver_ID')['Quarterly Rating'].transform('first')
df_new[df_new['Last_rat'] != df_new['first_rat']]

df_new['Last_income'] = df_new.groupby('Driver_ID')['Income'].transform('last')
df_new['first_income'] = df_new.groupby('Driver_ID')['Income'].transform('first')
df_new['change_income'] = df_new['Last_income'] - df_new['first_income']

df_new.drop(['Last_income','first_income'], axis = 1, inplace = True)

df_new['change_income'].describe()

df_new['Income_increased'] = np.where(df_new['change_income'] > 0.0, 1,0)

df_new['Grade_improved'] = np.where(df_new['Last_grade']- df_new['first_grade']> 0.0, 1,
                                    np.where(df_new['Last_grade']- df_new['first_grade']< 0.0, -1, 0))
df_new['Rate_improved'] = np.where(df_new['Last_rat']- df_new['first_rat']> 0.0, 1,
                                    np.where(df_new['Last_rat']- df_new['first_rat']< 0.0, -1, 0))

df_new['Rating_change'] = df_new['Last_rat']- df_new['first_rat']

df_new['Rating_change'].value_counts()

df_new[df_new['Rate_improved'] == -1]

df_new['Grade_improved'].value_counts()

df_new['income_change'] = np.where(df_new['change_income']>0.0, 1, 0)

df_new[['Age','Gender','Income','Grade','Churn','Income_increased','change_income','income_change','Rate_improved','Rating_change','Grade_improved']].corr()

df_new.drop(['first_grade','first_rat','change_income','income_change'],axis = 1,inplace = True)

df_new.columns

df_group = df_new.groupby('Driver_ID').agg({'MMM-YY':'max','Dateofjoining':'max','LastWorkingDate':'max',
                                 'Age':'max','Gender':'max','Income':'sum','Joining Designation':'max','Total Business Value':'sum','Education_Level':'max',
                                 'City_encoded':'max','Churn':'max','Last_grade':'max','Income_increased':'max','Last_rat':'max','Grade_improved':'max','Rating_change':'max'}).reset_index()

df_group['Time_served'] = np.where(df_group['LastWorkingDate'].isna(), np.abs(df_group['MMM-YY'] - df_group['Dateofjoining']), df_group['LastWorkingDate']- df_group['Dateofjoining'])

df_group['Time_served'] = df_group['Time_served'].dt.days

df_group['Income_increased'].value_counts(normalize = True)*100

"""Says if income is increased only 2% of Driver leaves"""

df_g = df_group.copy()

df_g.columns

df_g.drop(['MMM-YY','Dateofjoining','LastWorkingDate'],axis = 1,inplace = True)

n = list(df_g.columns)

df_g.info()

df_g['Churn'].value_counts(normalize = True)*100

"""67% Drivers Churn"""

for i in n:
  print(df_g[i].value_counts(normalize = True)*100)

"""58% are male
43% joined with designation 1, only 0.6% joined with designation 5

only 2% employees income & grade inproved
18% employees rating has been decreased over time
32% employee churn
"""

n

plt.figure(figsize = (10,7))
plt.subplot(231)
df_g['Gender'].value_counts(normalize = True).plot.bar(title = 'Gender')

plt.subplot(232)
df_g['Churn'].value_counts(normalize = True).plot.bar(title = 'Churn')

plt.subplot(233)
df_g['Education_Level'].value_counts(normalize = True).plot.bar(title = 'Education_Level')

plt.subplot(234)
df_g['Joining Designation'].value_counts(normalize = True).plot.bar()

"""Education level is constant"""

df_g['Churn'].value_counts()

plt.figure(figsize = (10,7))
plt.subplot(231)
sns.boxplot(x = df_g['Age'])

plt.subplot(232)
sns.boxplot(x = df_g['Income'])

plt.subplot(233)
sns.boxplot(x = df_g['Total Business Value'])

plt.subplot(234)
sns.boxplot(x = df_g['Time_served'])

pd.cut(df_g['Income'], bins = [0, 19900, 39900, 49900, 59900, 26990000])
#pd.crosstab(df_g[''])

"""High number of outliers in all the fields above

"""

df_g['income_bin'] = pd.cut(df_g['Income'],bins = [10000, 40000, 70000, 100000, 130000, 160000, 190000, 220000])
new = pd.crosstab(df_g['income_bin'],df_g['Churn']).reset_index()
new.plot(kind = 'bar')

df_g['Age_bin'] = pd.cut(df_g['Age'], bins = [30,35,40,45,50,55,60])
new = pd.crosstab(df_g['Age_bin'], df_g['Churn']).reset_index()
new.plot(kind = 'bar')

df_g.drop(['Age_bin','income_bin'], axis=1, inplace = True)

df_g.corr()

sns.countplot(x = df_g['Education_Level'], hue = df_g['Churn'])

"""Education level has no impact on churn"""

df_n = df_g.groupby('Last_rat')['Churn'].value_counts(normalize=True).unstack().reset_index()
df_n.melt(id_vars = 'Last_rat', value_name = 'p', var_name = 'Churn')

df_p = df_g.groupby('Last_rat')['Churn'].value_counts(normalize=True).unstack().reset_index()
df_p_new = df_p.melt(id_vars = 'Last_rat', value_name = 'p', var_name = 'Churn')

sns.barplot(x = 'Last_rat',y = 'p', hue = 'Churn', data = df_p_new)

"""almost 80% of drivers having 1 rating at last, have churned. And only 10% of drivers churn if rating is 4"""

df_g_r = df_g.groupby('Rating_change')['Churn'].value_counts(normalize = True).unstack().reset_index()
df_g_m = df_g_r.melt(id_vars = 'Rating_change', value_name = 'p',var_name = 'Churn')
sns.barplot(x = df_g_m['Rating_change'], y = df_g_m['p'], hue = df_g_m['Churn'])

"""if rating decreases driver churn is more, if rating increases drive churn is less"""

df_g_r = df_g.groupby('Income_increased')['Churn'].value_counts(normalize = True).unstack().reset_index()
df_g_m = df_g_r.melt(id_vars = 'Income_increased', value_name = 'p',var_name = 'Churn')
sns.barplot(x = df_g_m['Income_increased'], y = df_g_m['p'], hue = df_g_m['Churn'])

"""if income increases, churn is just 5%, and if it does not increasr chutn is 70%

with increasing salary churn rate is decreasing
"""

plt.subplot(231)
sns.histplot(df_g['Income'])

plt.subplot(232)
sns.histplot(df_g['Age'])

from scipy.stats import shapiro

stat, p = shapiro(df['Age'].dropna())
print(f"Shapiro-Wilk Test: p-value = {p}")

if p > 0.05:
    print("Data is likely normally distributed (fail to reject H0).")
else:
    print("Data is not normally distributed (reject H0).")

"""Skewness check"""

from scipy.stats import skew

skewness = skew(df['Age'].dropna())  # Drop NaN values before calculating
print(f"Skewness: {skewness}")

from scipy.stats import skew

skewness = skew(df['Income'].dropna())  # Drop NaN values before calculating
print(f"Skewness: {skewness}")

"""All age, income are right skewed"""

df_g['Time_bin']=pd.cut(df_g['Time_served'], bins = [0,200, 400, 600, 800, 1000, 1200, 1400, 1600,2000,4000])
df_g_r = df_g.groupby('Time_bin')['Churn'].value_counts(normalize = True).unstack().reset_index()
df_g_m = df_g_r.melt(id_vars = 'Time_bin', value_name = 'p',var_name = 'Churn')
plt.figure(figsize = (13,7))
sns.barplot(x = df_g_m['Time_bin'], y = df_g_m['p'], hue = df_g_m['Churn'])

"""Can not derive much info from time served

problem with gender due to KNN approach. Categorical field
"""

df_g['Gender'] = df_g['Gender'].round().astype(int)

"""## Build Model"""

df_g.columns

df_g.columns

X = df_g.drop(['Driver_ID','City_encoded','Time_served','Time_bin','Churn'], axis = 1)
y = df_g['Churn']

#X.drop(['Age_bin','income_bin'], axis = 1, inplace = True)

"""

---



Scaling"""

from sklearn.preprocessing import MinMaxScaler

X

scaler = MinMaxScaler()
X_cols = X.columns
X = pd.DataFrame(X, columns = X_cols)

X = scaler.fit_transform(X)

X_cols

"""

---
Split data
"""

x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)

"""

---

Balencing
We need balencing as Target is not normally distributed"""

#pip install imbalanced-learn

from imblearn.over_sampling import SMOTE

SMOTE(sampling_strategy = 'auto', random_state = 42)

smote = SMOTE(sampling_strategy = "auto", random_state = 42)

x_resample, y_resample = smote.fit_resample(x_train, y_train)
x_resample = pd.DataFrame(x_resample, columns=X_cols)

"""# Build Decision Tree"""

from sklearn.metrics import confusion_matrix, classification_report

from sklearn.tree import DecisionTreeClassifier

# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clft = clf.fit(x_resample,y_resample)

#Predict the response for test dataset
y_pred = clft.predict(x_test)

print(classification_report(y_test, y_pred))

print(classification_report( y_pred, y_test))

param = ({'ccp_alpha':[7,8,9], 'n_estimators':[67,80]})
rf = RandomForestClassifier()
c= GridSearchCV(rf, param, n_jobs = 1, cv = 3, scoring = 'f1', )

"""# Random Forest"""

param = ({'ccp_alpha':[0.01,0.02], 'n_estimators':[100,150]})
rf= RandomForestClassifier()
clf1 = GridSearchCV(rf,param,n_jobs = 1,cv = 3, scoring = 'f1', refit = True)

clf1.fit(x_resample, y_resample)
print(clf1.best_params_)


y_pred = clf1.predict(x_test)

clf1.best_score_

print(classification_report(y_test,y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#XGBoost"""

import xgboost as xgb

param = ({'n_estimators':[400],'max_depth':[4,5],  'learning_rate': [0.01],
          'reg_alpha': [0, 0.001, 0.01],
      'reg_lambda': [0, 0.001, 0.01]  })

xgbb = xgb.XGBClassifier()
xg = GridSearchCV(xgbb, param, n_jobs = 1, cv =3, scoring = 'f1')
xg.fit(x_resample, y_resample)
print(xg.best_params_)
print(xg.best_score_)
y_pred = xg.predict(x_test)
print(classification_report(y_test,y_pred))

# Ensure x_test is a DataFrame (not a NumPy array)
x_test_df = pd.DataFrame(x_test, columns=X_cols)  # X should be your original feature DataFrame

# SHAP explanation
explainer = shap.TreeExplainer(xg.best_estimator_)
shap_values = explainer.shap_values(x_test_df)

# Get top 3 contributing features for the first test sample
top_features = pd.Series(shap_values[0], index=x_test_df.columns).abs().sort_values(ascending=False).head(3)
print(top_features)

import pickle

# Save model
with open("xgb_model.pkl", "wb") as f:
    pickle.dump(xg, f)

with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

with open("xgb_explainer.pkl", "wb") as f:
    pickle.dump(explainer, f)

#from google.colab import files

# Download model
#files.download('xgb_model.pkl')

# Download scaler
#files.download('scaler.pkl')

# Download explainer (optional)
#files.download('xgb_explainer.pkl')

"""# LightGBM"""

#!pip install lightgbm

#import lightgbm as lgb

#train_data = lgb.Dataset(x_resample, label=y_resample)
#test_data = lgb.Dataset(x_test, label=y_test)
params = {
    'objective': 'binary',  # Change to 'multiclass' if multi-class
    'metric': 'accuracy',
    'boosting_type': 'gbdt',  # Gradient Boosting Decision Trees
    'num_leaves': 31,  # Control tree complexity
    'learning_rate': 0.05,
    'n_estimators': 100,
    'verbose': -1
}

#model = lgb.train(params, train_data, valid_sets=[test_data])
#y_pred_prob = model.predict(x_test)  # Get probabilities
#y_pred = np.where(y_pred_prob > 0.5, 1, 0)  # Convert to binary labels
#print("Accuracy:", accuracy_score(y_test, y_pred))
#print(classification_report(y_test, y_pred))

"""LightGBM gave equially good result with very less time

Out of actual churn count we predicted 86% churn correctly. (recall). and out of our prediction of churn 83% is correct (precision).

as we see random forest gave slightly better recall and precision than xgboost here.

# Feature Importance
"""

#pip install shap



import shap

explainer = shap.Explainer(model, x_resample)
shap_values = explainer(x_test)  # SHAP values for test data

shap.summary_plot(shap_values, x_test)

shap.summary_plot(shap_values, x_test, plot_type="bar")

"""xgb.plot_importance(best_mode) plt.show"""

import matplotlib.pyplot as plt
import xgboost as xgb

# Get the best model from GridSearchCV
best_model = xg.best_estimator_

# Plot feature importance
xgb.plot_importance(best_model)
plt.show()

feature_importance = best_model.feature_importances_
feature_names = x_resample.columns  # Ensure your dataset has column names

# Create a DataFrame and sort values
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print(importance_df)

import shap

# Explain the best XGBoost model
explainer = shap.Explainer(best_model, x_resample)
shap_values = explainer(x_test)

# Summary plot (Global Feature Importance)
#shap.summary_plot(shap_values, x_test)

"""Last rating, change in rating, income, Total business value and joining designation has high imporatance in prediction of churn"""

